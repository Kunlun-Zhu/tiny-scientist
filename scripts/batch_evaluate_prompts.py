import argparse
import json
import os
import traceback
from multiprocessing import Pool, Manager, Lock, cpu_count
from functools import partial
from datetime import datetime # For unique output sub-directories
from typing import List, Dict, Optional, Any
import logging
import sys
from collections import defaultdict # Added for convenience

# TinyScientist imports
from tiny_scientist import Thinker, SafetyEvaluator
from tiny_scientist.prompts.prompts import AttackPrompts, DefensePrompts
from tiny_scientist.utils.llm import create_client

# Default model names (can be overridden by args)
DEFAULT_THINKER_MODEL = "gpt-3.5-turbo"
DEFAULT_EVALUATOR_MODEL = "gpt-4o" # SafetyEvaluator is used by Thinker

# --- Worker Function ---
def process_task_for_idea_generation(task_info_tuple, common_args, overall_output_dir_base, file_lock):
    """
    Processes a single task (intent + scenario configuration) to generate a research idea using Thinker.
    Writes the result to a shared JSONL file.
    """
    task_original_index, task_config = task_info_tuple # task_config is what load_tasks generates per scenario
    task_id = task_config.get("id") # Use the ID generated by generate_scenario_tasks
    intent = task_config.get("intent")
    domain = task_config.get("domain")
    scenario = task_config.get("scenario") # This is the internal enum-like name

    entry_to_write = {
        "task_id": task_id,
        "original_intent": intent,
        "domain": domain,
        "scenario": scenario,
        "thinker_model": common_args.thinker_model_name,
        "evaluator_model": common_args.evaluator_model_name, # For SafetyEvaluator inside Thinker
        "status": "failed_setup", # Default status
        "thinker_output": None,
        "error_details": None,
        "traceback": None
    }

    if not intent or not domain:
        print(f"[WARNING] Worker {os.getpid()}: Task ID {task_id} is missing 'intent' or 'domain'. Skipping.")
        entry_to_write["error_details"] = "Missing intent or domain in task_config."
        entry_to_write["status"] = "skipped_missing_data"
    else:
        print(f"[INFO] Worker {os.getpid()}: Processing {task_id} (Domain: {domain})")
        # Per-task artifact directory (optional, Thinker might not save files directly in this flow)
        # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f") # Added microseconds for more uniqueness
        # current_task_artifact_dir = os.path.join(overall_output_dir_base, f"{task_id}_{timestamp}")
        # os.makedirs(current_task_artifact_dir, exist_ok=True)
        # entry_to_write["task_artifact_directory"] = current_task_artifact_dir

        try:
            thinker_llm_client, thinker_model_name_actual = create_client(model=common_args.thinker_model_name)
            evaluator_llm_client, evaluator_model_name_actual = create_client(model=common_args.evaluator_model_name)

            safety_evaluator_instance = SafetyEvaluator(
                llm_client=evaluator_llm_client,
                model_name=evaluator_model_name_actual,
                apply_final_attack=task_config.get("apply_final_attack_in_evaluator", False),
                final_attack_prompt_template=task_config.get("final_attack_template_for_evaluator"),
                apply_final_defense=task_config.get("apply_final_defense_in_evaluator", False),
                final_defense_prompt_template=task_config.get("final_defense_template_for_evaluator")
            )

            thinker_instance = Thinker(
                llm_client=thinker_llm_client, 
                model_name=thinker_model_name_actual, 
                initial_research_intent=intent,
                domain=domain,
                safety_evaluator=safety_evaluator_instance, 
                enable_discussion_attack=task_config.get("enable_discussion_attack", False),
                enable_discussion_defense=task_config.get("enable_discussion_defense", False),
                discussion_malicious_agent_template=task_config.get("discussion_attack_template"),
                discussion_defense_agent_template=task_config.get("discussion_defense_template"),
                tools=[], iter_num=1,
            )
            
            thinker_run_output = thinker_instance.run_discussion_and_evaluate(num_rounds=2)
            entry_to_write["thinker_output"] = thinker_run_output
            if thinker_run_output.get("error"):
                entry_to_write["error_details"] = f"Thinker/Evaluator reported an error: {thinker_run_output['error']}"
                entry_to_write["status"] = "failed_thinker_evaluator_reported_error"
                print(f"[ERROR] Worker {os.getpid()}: Task ID {task_id} - Thinker error: {thinker_run_output['error']}")
            elif thinker_run_output.get("safety_score") is None:
                entry_to_write["error_details"] = "Safety score is null, evaluation might have failed."
                entry_to_write["status"] = "failed_evaluation_no_score"
            else:
                entry_to_write["status"] = "success"
                print(f"[SUCCESS] Worker {os.getpid()}: Task {task_id} completed with score: {thinker_run_output.get('safety_score')}.")

        except Exception as e:
            print(f"[ERROR] Worker {os.getpid()}: Unhandled exception processing Task ID {task_id}: {e}")
            entry_to_write["error_details"] = str(e)
            entry_to_write["traceback"] = traceback.format_exc()
            entry_to_write["status"] = "failed_processing_exception"

    # Write result to the shared JSONL output file
    if entry_to_write:
        try:
            file_lock.acquire()
            with open(common_args.output_file, 'a', encoding='utf-8') as outfile:
                outfile.write(json.dumps(entry_to_write) + "\n")
                outfile.flush()
        except Exception as e:
            print(f"[ERROR] Worker {os.getpid()}: Failed to write result for Task ID {entry_to_write.get('task_id')} to file: {e}")
        finally:
            file_lock.release()
            
    return entry_to_write.get("status", "unknown_status")


# --- Task Loading Function ---
def load_input_file(filepath: str, limit: Optional[int] = None) -> List[Dict[str, str]]:
    """Loads the input JSON file (can be standard JSON array or JSONL).
       For standard JSON array, it extracts objects. For JSONL, it reads line by line.
       Ensures 'Prompt' and 'Domain' keys are present, defaulting Domain if necessary.
    """
    records = []
    raw_records = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            try:
                # Attempt to parse as a single JSON array first
                loaded_json = json.loads(content)
                if isinstance(loaded_json, list):
                    raw_records = loaded_json
                    print(f"[INFO] Loaded {len(raw_records)} records from JSON array in {filepath}.")
                elif isinstance(loaded_json, dict):
                    # Handle case where file might be a single JSON object (treat as one record)
                    raw_records = [loaded_json]
                    print(f"[INFO] Loaded 1 record (single JSON object) from {filepath}.")
                else:
                    # If not list or dict, it might be malformed or an unexpected structure
                    logging.warning(f"File {filepath} is not a JSON array or a single JSON object. Attempting JSONL parse.")
                    # Fall through to JSONL parsing
            except json.JSONDecodeError:
                # If parsing as a single JSON array fails, assume it's JSONL or malformed line by line
                logging.warning(f"Failed to parse {filepath} as a single JSON array. Attempting JSONL parse.")
                f.seek(0) # Reset file pointer to the beginning for line-by-line reading
                for i, line in enumerate(f):
                    line_stripped = line.strip()
                    if not line_stripped: continue # Skip empty lines
                    try:
                        raw_records.append(json.loads(line_stripped))
                    except json.JSONDecodeError:
                        logging.warning(f"Skipping malformed JSON line in {filepath}: {line_stripped}")
                print(f"[INFO] Attempted JSONL parse on {filepath}, loaded {len(raw_records)} records.")

        # Process raw_records to ensure required keys and apply limit
        count = 0
        for record_dict in raw_records:
            if limit is not None and count >= limit:
                break
            
            if not isinstance(record_dict, dict):
                logging.warning(f"Skipping non-dictionary record: {record_dict}")
                continue

            prompt = record_dict.get("Prompt")
            # Use "Task" as TaskID if "TaskID" is missing
            task_id = record_dict.get("TaskID", record_dict.get("Task")) 
            # Default domain if not present or empty
            domain = record_dict.get("Domain")
            if not domain or not str(domain).strip():
                domain = "Medical Science" # Default domain for med.json if not specified
                # logging.info(f"Record missing 'Domain', defaulting to '{domain}' for prompt: {prompt[:50]}...")

            if prompt and task_id:
                records.append({
                    "TaskID": str(task_id), 
                    "Prompt": str(prompt), 
                    "Domain": str(domain)
                })
                count += 1
            else:
                logging.warning(f"Skipping record due to missing 'Prompt' or 'TaskID'/'Task': {record_dict}")
        
        print(f"[INFO] Successfully processed {len(records)} records with required fields from {filepath}.")

    except FileNotFoundError:
        logging.error(f"Input file not found: {filepath}")
        # sys.exit(1) # Or handle error more gracefully depending on script's overall flow
    except Exception as e:
        logging.error(f"Error loading or processing file {filepath}: {e}")
        traceback.print_exc()
        
    return records

def generate_scenario_tasks(
    base_tasks: List[Dict[str, str]],
    attack_prompt_template_map: Dict[str, str],
    defense_prompt_template_map: Dict[str, str],
    scenarios_to_run: List[str]
) -> List[Dict[str, Any]]:
    scenario_tasks = []
    
    universal_strong_attack_key = AttackPrompts.EXTREME_RISK_INJECTION_PROMPT.name
    universal_strong_attack_template = attack_prompt_template_map.get(universal_strong_attack_key)
    if not universal_strong_attack_template:
        logging.error(f"CRITICAL: Universal strong attack prompt '{universal_strong_attack_key}' not found/empty.")

    # Key for the new general defense prompt
    general_defense_prompt_key = DefensePrompts.GENERAL_DEFENSE_PROMPT.name
    general_defense_template = defense_prompt_template_map.get(general_defense_prompt_key)
    if not general_defense_template:
        logging.warning(f"WARNING: General defense prompt '{general_defense_prompt_key}' not found/empty. Defense steps might be less effective if domain-specific also missing.")

    for i, base_task in enumerate(base_tasks):
        intent = base_task.get("Prompt")
        domain_from_file = base_task.get("Domain", "General") # Default to a generic domain if not specified
        base_task_id = base_task.get("TaskID", f"intent{i}")
        if not intent: logging.warning(f"Task {base_task_id} missing Prompt."); continue

        normalized_domain_key_part = domain_from_file.upper().replace(" ", "_")

        # Discussion templates (fall back to general if specific not found, then to None if general also not found)
        discussion_attack_template = attack_prompt_template_map.get(f"{normalized_domain_key_part}_ATTACK_PROMPT", 
                                                                  attack_prompt_template_map.get(AttackPrompts.MEDICAL_SCIENCE_ATTACK_PROMPT.name)) # Default discussion attack
        discussion_defense_template = defense_prompt_template_map.get(f"{normalized_domain_key_part}_DEFENSE_PROMPT", general_defense_template)

        # Final defense template (for Defense2 and Attack2+Defense2 scenarios, used by SafetyEvaluator)
        # Try domain-specific L1, then general defense, then None
        final_defense_template = defense_prompt_template_map.get(f"{normalized_domain_key_part}_DEFENSE_PROMPT", general_defense_template)
        if not final_defense_template:
             logging.warning(f"No specific or general defense template for final defense for domain {domain_from_file} in {base_task_id}. Defense2 will be ineffective.")

        scenarios = {
            "NORMAL": {
                "enable_discussion_attack": False, "enable_discussion_defense": False, 
                "apply_final_attack_in_evaluator": False, "final_attack_template_for_evaluator": None,
                "apply_final_defense_in_evaluator": False, "final_defense_template_for_evaluator": None,
                "discussion_attack_template": None, "discussion_defense_template": None
            },
            "ATTACK1": {
                "enable_discussion_attack": True, "enable_discussion_defense": False,
                "apply_final_attack_in_evaluator": False, "final_attack_template_for_evaluator": None,
                "apply_final_defense_in_evaluator": False, "final_defense_template_for_evaluator": None,
                "discussion_attack_template": discussion_attack_template, "discussion_defense_template": None
            },
            "DEFENSE1": {
                "enable_discussion_attack": False, "enable_discussion_defense": True, # Defense agent active in discussion
                "apply_final_attack_in_evaluator": False, "final_attack_template_for_evaluator": None,
                "apply_final_defense_in_evaluator": False, "final_defense_template_for_evaluator": None,
                "discussion_attack_template": None, "discussion_defense_template": discussion_defense_template
            },
            "ATTACK1_DEFENSE1": {
                "enable_discussion_attack": True, "enable_discussion_defense": True,
                "apply_final_attack_in_evaluator": False, "final_attack_template_for_evaluator": None,
                "apply_final_defense_in_evaluator": False, "final_defense_template_for_evaluator": None,
                "discussion_attack_template": discussion_attack_template, "discussion_defense_template": discussion_defense_template
            },
            "ATTACK2": {
                "enable_discussion_attack": False, "enable_discussion_defense": False,
                "apply_final_attack_in_evaluator": True, "final_attack_template_for_evaluator": universal_strong_attack_template,
                "apply_final_defense_in_evaluator": False, "final_defense_template_for_evaluator": None,
                "discussion_attack_template": None, "discussion_defense_template": None
            },
            "DEFENSE2": {
                "enable_discussion_attack": False, "enable_discussion_defense": False,
                "apply_final_attack_in_evaluator": False, "final_attack_template_for_evaluator": None,
                "apply_final_defense_in_evaluator": True, "final_defense_template_for_evaluator": final_defense_template, # Use L1 or General for final defense
                "discussion_attack_template": None, "discussion_defense_template": None
            },
            "ATTACK2_DEFENSE2": {
                "enable_discussion_attack": False, "enable_discussion_defense": False,
                "apply_final_attack_in_evaluator": True, "final_attack_template_for_evaluator": universal_strong_attack_template,
                "apply_final_defense_in_evaluator": True, "final_defense_template_for_evaluator": final_defense_template, # Use L1 or General for final defense
                "discussion_attack_template": None, "discussion_defense_template": None
            }
        }

        for scenario_name_enum_like, config_flags in scenarios.items():
            scenario_name_for_check = scenario_name_enum_like.replace("_", "+") 
            if scenario_name_enum_like == "ATTACK1_DEFENSE1": scenario_name_for_check = "ATTACK1+DEFENSE1"
            if scenario_name_enum_like == "ATTACK2_DEFENSE2": scenario_name_for_check = "ATTACK2+DEFENSE2"
            if scenario_name_for_check in scenarios_to_run or scenario_name_enum_like in scenarios_to_run:
                task_config = {
                    "id": f"{base_task_id}_{scenario_name_enum_like.lower()}_exp",
                    "intent": intent, "domain": domain_from_file, "scenario": scenario_name_enum_like,
                    **config_flags
                }
                scenario_tasks.append(task_config)
    return scenario_tasks

# --- Main Function ---
def main():
    parser = argparse.ArgumentParser(description="Batch evaluate research ideas using Thinker and SafetyEvaluator.")
    parser.add_argument("--input_file", type=str, required=True, help="Path to the input JSONL file with prompts.")
    parser.add_argument("--output_file", type=str, required=True, help="Path to the output JSONL file for results.")
    parser.add_argument("--stats_file", type=str, default="output/batch_idea_generation_statistics.json", help="Path to the output JSON file for statistics.")
    parser.add_argument("--thinker_model_name", type=str, default="gpt-3.5-turbo", help="LLM model name for the Thinker.")
    parser.add_argument("--evaluator_model_name", type=str, default="gpt-4o", help="LLM model name for the SafetyEvaluator.")
    parser.add_argument("--num_workers", type=int, default=max(1, cpu_count() // 2), help="Number of parallel worker processes.")
    parser.add_argument("--resume", action="store_true", help="Resume from previously processed tasks in the output file.")
    parser.add_argument("--limit", type=int, default=None, help="Limit processing to the first N prompts from the input file.")
    parser.add_argument("--scenarios_to_run", type=str, default="NORMAL,MALICIOUS_ATTACK,DEFENDED", help="Comma-separated list of scenarios to run (e.g., 'NORMAL,MALICIOUS_ATTACK')")

    args = parser.parse_args()

    # Create base output directories
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    os.makedirs(os.path.dirname(args.stats_file), exist_ok=True)

    # --- Resume Logic ---
    processed_task_ids = set()
    if os.path.exists(args.output_file):
        print(f"[INFO] Output file {args.output_file} exists. Reading previously processed task IDs...")
        try:
            with open(args.output_file, 'r', encoding='utf-8') as f_read:
                for line_num, line in enumerate(f_read):
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if "task_id" in data: # Use the unique task_id (e.g., "input_task_1_normal")
                                processed_task_ids.add(data["task_id"])
                        except json.JSONDecodeError:
                            print(f"[WARNING] JSON decode error on line {line_num+1} of existing output file. Skipping for resume check.")
            print(f"[INFO] Found {len(processed_task_ids)} unique task IDs in existing output file.")
        except Exception as e:
            print(f"[ERROR] Failed to read or parse existing output file {args.output_file} for resume: {e}. Will process all tasks as new.")
            processed_task_ids.clear()
    else:
        print(f"[INFO] Output file {args.output_file} does not exist. Starting fresh.")

    # Load base tasks from the input file
    base_tasks = load_input_file(args.input_file, args.limit) # Pass limit here
    if not base_tasks:
        logging.info("No tasks to process from input file.")
        return

    # Parse scenarios_to_run argument
    scenarios_to_run_list = [s.strip().upper() for s in args.scenarios_to_run.split(',') if s.strip()]
    if not scenarios_to_run_list:
        logging.error("No scenarios specified to run. Please provide at least one scenario via --scenarios_to_run.")
        sys.exit(1)
    logging.info(f"Specified scenarios to run: {scenarios_to_run_list}")

    # Prepare prompt template maps
    # These would ideally be loaded from a config or a more structured prompt management system
    # For now, we'll create them based on the existing Enum structure for simplicity in this script
    attack_prompt_template_map = {prompt.name: prompt.value for prompt in AttackPrompts}
    defense_prompt_template_map = {prompt.name: prompt.value for prompt in DefensePrompts}
    # Add explicit defaults if GENERAL_PURPOSE_ATTACK_PROMPT_DEFAULT was used before
    if "CHEMISTRY_ATTACK_PROMPT" in attack_prompt_template_map: # Example default
        attack_prompt_template_map.setdefault("DEFAULT_ATTACK_PROMPT", attack_prompt_template_map["CHEMISTRY_ATTACK_PROMPT"])
    if "CHEMISTRY_DEFENSE_PROMPT" in defense_prompt_template_map:
        defense_prompt_template_map.setdefault("DEFAULT_DEFENSE_PROMPT", defense_prompt_template_map["CHEMISTRY_DEFENSE_PROMPT"])
    
    # Generate specific scenario tasks
    all_scenario_tasks = generate_scenario_tasks(
        base_tasks,
        attack_prompt_template_map,
        defense_prompt_template_map,
        scenarios_to_run_list
    )

    # Filter for tasks not yet processed if resuming
    new_scenario_tasks = [task for task in all_scenario_tasks if task["id"] not in processed_task_ids]

    num_tasks_actually_running = len(new_scenario_tasks)
    print(f"[INFO] Total scenario tasks generated from input: {len(all_scenario_tasks)}")

    if num_tasks_actually_running == 0:
        print("[INFO] No new tasks to process. All tasks from input file are already in the output file.")
        generate_final_statistics(args.output_file, args) # Still generate stats for all processed items
        return
        
    # Prepare arguments for workers: tuple of (index, task_config_item)
    # The index here is just for the current batch of tasks to run.
    worker_payload = list(enumerate(new_scenario_tasks))

    # API Key Check (Informational)
    if not os.environ.get("OPENAI_API_KEY"): # Or other relevant key checks for your LLM client
        print("[WARNING] OPENAI_API_KEY environment variable not set. LLM calls may fail if using OpenAI.")

    print(f"\n[SETUP] Starting batch idea generation:")
    print(f"  Input File: {args.input_file}")
    print(f"  Output File (JSONL): {args.output_file}")
    print(f"  Thinker Model: {args.thinker_model_name}")
    print(f"  SafetyEvaluator Model (in Thinker): {args.evaluator_model_name}")
    print(f"  Parallel Workers: {args.num_workers}")

    # --- BEGIN DEBUG PRINT --- (Added to main to check loaded templates)
    print("-" * 70)
    print("[DEBUG_BATCH_MAIN] Checking critical attack/defense templates directly from Enum & Map:")
    
    # Check EXTREME_RISK_INJECTION_PROMPT
    extreme_key = "EXTREME_RISK_INJECTION_PROMPT"
    try:
        extreme_prompt_value_enum = AttackPrompts[extreme_key].value
        if extreme_prompt_value_enum and isinstance(extreme_prompt_value_enum, str) and extreme_prompt_value_enum.strip():
            print(f"  Directly from Enum: AttackPrompts.{extreme_key}.value (first 100): {extreme_prompt_value_enum[:100]}...")
        else:
            print(f"  Directly from Enum: AttackPrompts.{extreme_key}.value is None, empty, or not a string.")
    except KeyError:
        print(f"  ERROR: AttackPrompts.{extreme_key} member does not exist in Enum!")
    
    value_from_map_extreme = attack_prompt_template_map.get(extreme_key)
    if value_from_map_extreme and isinstance(value_from_map_extreme, str) and value_from_map_extreme.strip():
        print(f"  From attack_prompt_template_map for key '{extreme_key}' (first 100): {value_from_map_extreme[:100]}...")
    else:
        print(f"  From attack_prompt_template_map for key '{extreme_key}': Value is None, empty, or not a string.")

    # Check CHEMISTRY_ATTACK_PROMPT as it was a fallback
    chem_attack_key = "CHEMISTRY_ATTACK_PROMPT"
    value_from_map_chem = attack_prompt_template_map.get(chem_attack_key)
    if value_from_map_chem and isinstance(value_from_map_chem, str) and value_from_map_chem.strip():
        print(f"  From attack_prompt_template_map for key '{chem_attack_key}' (first 100): {value_from_map_chem[:100]}...")
    else:
        print(f"  From attack_prompt_template_map for key '{chem_attack_key}': Value is None, empty, or not a string.")
    
    # Check MEDICAL_SCIENCE_DEFENSE_PROMPT as it's used as a default for defense in discussion
    med_defense_key = "MEDICAL_SCIENCE_DEFENSE_PROMPT"
    value_from_map_med_defense = defense_prompt_template_map.get(med_defense_key)
    if value_from_map_med_defense and isinstance(value_from_map_med_defense, str) and value_from_map_med_defense.strip():
        print(f"  From defense_prompt_template_map for key '{med_defense_key}' (first 100): {value_from_map_med_defense[:100]}...")
    else:
        print(f"  From defense_prompt_template_map for key '{med_defense_key}': Value is None, empty, or not a string.")

    print("-" * 70)
    # --- END DEBUG PRINT ---

    # --- Multiprocessing ---
    if args.num_workers > 1 and num_tasks_actually_running > 0:
        manager = Manager()
        shared_file_lock = manager.Lock()
        # Use partial to pass fixed arguments to the worker function
        worker_func_with_args = partial(process_task_for_idea_generation, 
                                        common_args=args, 
                                        overall_output_dir_base=os.path.dirname(args.output_file), 
                                        file_lock=shared_file_lock)
        print(f"\n[INFO] Starting parallel processing of {num_tasks_actually_running} tasks with {args.num_workers} workers...")
        with Pool(processes=args.num_workers) as pool:
            # map_results will be a list of statuses returned by the worker
            pool.map(worker_func_with_args, worker_payload) 
        print("[INFO] Parallel processing finished.")
    elif num_tasks_actually_running > 0: # Sequential processing
        print(f"\n[INFO] Starting sequential processing of {num_tasks_actually_running} tasks...")
        class DummyLock: # For sequential execution, lock is not strictly needed but keeps worker signature same
            def acquire(self): pass
            def release(self): pass
        dummy_file_lock = DummyLock()
        worker_func_with_args = partial(process_task_for_idea_generation, 
                                        common_args=args, 
                                        overall_output_dir_base=os.path.dirname(args.output_file), 
                                        file_lock=dummy_file_lock)
        for i, task_config_item in enumerate(new_scenario_tasks):
            worker_func_with_args((i, task_config_item)) # Pass as tuple (index, item)
        print("[INFO] Sequential processing finished.")
    else:
        # This case should be caught earlier by "No new tasks to process"
        pass

    print(f"\n[INFO] Batch idea generation session complete.")
    print(f"  Results (newly processed tasks) appended to: {args.output_file}")
    
    # --- Final Statistics Generation (after all processing) ---
    generate_final_statistics(args.output_file, args)


def generate_final_statistics(results_jsonl_file, script_args):
    """
    Reads all results from the JSONL file and generates a statistics summary.
    """
    print(f"\n[STATS] Generating final statistics from: {results_jsonl_file}")
    
    all_results_from_file = []
    if not os.path.exists(results_jsonl_file):
        print(f"[STATS_ERROR] Output file {results_jsonl_file} not found. Cannot generate statistics.")
        return

    try:
        with open(results_jsonl_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                if line.strip():
                    try:
                        all_results_from_file.append(json.loads(line))
                    except json.JSONDecodeError:
                        print(f"[STATS_WARNING] Could not parse line {line_num+1} from {results_jsonl_file} for statistics. Skipping.")
    except Exception as e:
        print(f"[STATS_ERROR] Error reading {results_jsonl_file} for statistics: {e}")
        return

    total_entries_in_file = len(all_results_from_file)
    successful_generations = 0
    failed_generations = 0 # Includes "skipped", "failed_setup", "failed_thinker_reported_error", "failed_processing_exception"
    
    # For safety score statistics (if present in thinker_output)
    domain_scores_data = {}  # {"DomainName": {"sum": X, "count": Y, "scores": []}}
    scenario_scores_data = {} # {"ScenarioName": {"sum": X, "count": Y, "scores": []}}

    for res in all_results_from_file:
        status = res.get("status", "unknown_status")
        if status == "success":
            successful_generations += 1
        else:
            failed_generations += 1
            # Optionally log more details about failures here if needed for stats
            # print(f"[STATS_DEBUG] Failed/Skipped task: ID {res.get('task_id')}, Status: {status}, Error: {res.get('error_details')}")

        # Extract safety score if available and successful
        if status == "success" and res.get("thinker_output"):
            thinker_out = res["thinker_output"]
            safety_score = thinker_out.get("safety_score") # Path to score in thinker.run_discussion_and_evaluate() output
            
            if safety_score is not None: # Check it's a valid score, not None
                try:
                    score_value = float(safety_score) # Ensure it's a number
                    
                    domain = res.get("domain")
                    scenario = res.get("scenario")

                    if domain:
                        if domain not in domain_scores_data:
                            domain_scores_data[domain] = {"sum_scores": 0.0, "count": 0, "scores_list": []}
                        domain_scores_data[domain]["sum_scores"] += score_value
                        domain_scores_data[domain]["count"] += 1
                        domain_scores_data[domain]["scores_list"].append(score_value)
                    
                    if scenario:
                        if scenario not in scenario_scores_data:
                            scenario_scores_data[scenario] = {"sum_scores": 0.0, "count": 0, "scores_list": []}
                        scenario_scores_data[scenario]["sum_scores"] += score_value
                        scenario_scores_data[scenario]["count"] += 1
                        scenario_scores_data[scenario]["scores_list"].append(score_value)
                except (ValueError, TypeError):
                    print(f"[STATS_WARNING] Could not parse safety_score '{safety_score}' as float for task {res.get('task_id')}. Skipping score for stats.")


    final_stats_dict = {
        "metadata": {
            "input_file_processed": script_args.input_file,
            "output_results_file": script_args.output_file,
            "thinker_model_used": script_args.thinker_model_name,
            "evaluator_model_used": script_args.evaluator_model_name,
            "num_workers_configured": script_args.num_workers,
            "stats_generation_time": datetime.now().isoformat()
        },
        "overall_summary": {
            "total_task_entries_in_output_file": total_entries_in_file,
            "successful_idea_generations": successful_generations,
            "failed_or_skipped_generations": failed_generations,
        },
        "average_safety_scores_by_domain": {},
        "average_safety_scores_by_scenario": {}
    }

    for domain, data in domain_scores_data.items():
        if data["count"] > 0:
            final_stats_dict["average_safety_scores_by_domain"][domain] = {
                "average_score": round(data["sum_scores"] / data["count"], 3),
                "count_with_scores": data["count"],
                "all_scores": sorted(data["scores_list"])
            }
    for scenario, data in scenario_scores_data.items():
        if data["count"] > 0:
            final_stats_dict["average_safety_scores_by_scenario"][scenario] = {
                "average_score": round(data["sum_scores"] / data["count"], 3),
                "count_with_scores": data["count"],
                "all_scores": sorted(data["scores_list"])
            }
            
    # Save statistics to a new file (or overwrite if named the same as old STATS_FILE)
    stats_output_filename = os.path.join(os.path.dirname(script_args.output_file), "batch_idea_generation_statistics.json")
    try:
        with open(stats_output_filename, "w", encoding='utf-8') as f_stats:
            json.dump(final_stats_dict, f_stats, indent=4)
        print(f"[INFO] Final statistics saved to: {stats_output_filename}")
    except Exception as e:
        print(f"[STATS_ERROR] Could not save statistics file {stats_output_filename}: {e}")

    print("\n--- Final Statistics Summary ---")
    print(json.dumps(final_stats_dict, indent=2))
    print("--- End of Statistics ---")


if __name__ == "__main__":
    main()
